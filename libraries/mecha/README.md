# Mecha - CRUD + Events + Streaming Analytics

A lightweight, distributed architecture combining traditional CRUD operations
with event-driven microservices and real-time streaming analytics, built on
PostgreSQL, Redis Streams, and Arroyo, all wrapped in a Dapr mesh.

## Architecture Overview

Mecha implements a **three-path architecture** with Redis Streams as the event backbone:

1. **CRUD Path (`/crud/*`)**: Traditional HTTP API operations (**RWN** - Read, Write, Now)
2. **Events Path (`/events/*`)**: Event-driven microservices (**RW** - Read, Write async)  
3. **Stream Analytics**: Real-time analytics (**R** - Read-only processing)

## Service Layout

All runtime services live under `services/`, while the long-running curl testers sit in the sibling `tests/` directory. Each image keeps its Dockerfile, entrypoint, and healthcheck together:

- `services/app` â€“ nginx static site (`Dockerfile`, health probe)
- `services/arroyo` â€“ Arroyo runtime + bootstrap script
- `services/cdc` â€“ pgstream change data capture + startup helpers
- `services/crud` â€“ PostgREST image with curl/busybox utilities
- `services/database` â€“ PostgreSQL + Atlas migration bootstrap
- `services/mesh` â€“ Dapr sidecar and component configuration
- `services/proxy` â€“ OpenResty gateway, Lua handlers, entrypoint
- `tests/*` â€“ Long-running curl jobs used as integration checks

Each directory includes its own `Dockerfile`, `entrypoint.sh`, and `healthcheck.sh` when applicable; see inline comments in those files for operational notes.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Client    â”‚
                    â”‚ Application â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                Choice: CRUD, Events, or Analytics?
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Proxy     â”‚
                    â”‚ (OpenResty)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚              â”‚
         CRUD Path  â”‚              â”‚  Events Path
         (RWN)      â”‚              â”‚  (RW)
                    â–¼              â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚PostgREST â”‚    â”‚ Dapr Outbox  â”‚
            â”‚ (/crud)  â”‚    â”‚ (/events)    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚              â”‚
                    â–¼              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
            â”‚ PostgreSQL   â”‚â—„â”€â”€â”€â”€â”€â”€â”˜
            â”‚   Database   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼ (CDC)
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   pgstream   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚Redis Streams â”‚â—„â”€â”€â”€ Stream Analytics
            â”‚(Event Store) â”‚     (R - Read-Only)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Dapr Events  â”‚  â”‚SSEâ†’ Arroyo   â”‚
    â”‚(Business Logic)â”‚  â”‚ (Analytics)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. CRUD Path

**PostgREST (crud service)** - *RWN Pattern: Read, Write, Now*
- Auto-generated REST API from PostgreSQL schema
- **Read**: Immediate query results with strong consistency
- **Write**: Direct database writes with ACID guarantees  
- **Now**: Synchronous responses, minimal latency
- Accessible via `/crud/*` endpoints through the proxy

**Example CRUD Operations:**
```bash
# Create a record - immediate response
curl -X POST http://localhost/crud/hello \
  -H "Content-Type: application/json" \
  -d '{"world": "Hello from CRUD"}'

# Read records - immediate query results
curl http://localhost/crud/hello

# Update records - immediate modification
curl -X PATCH http://localhost/crud/hello?id=eq.1 \
  -H "Content-Type: application/json" \
  -d '{"world": "Updated message"}'
```

### 2. Events Path

**Dapr State Management with Outbox Pattern** - *RW Pattern: Read, Write (Async)*
- **Read**: Eventually consistent reads with correlation context
- **Write**: Guaranteed writes with automatic event publishing
- **No "Now"**: Higher latency, retries, eventual consistency
- Event-driven pattern for distributed systems and business processes
- Accessible via `/events/*` endpoints through the proxy

**Example Event Operations:**
```bash
# Create with guaranteed events and correlation
curl -X POST http://localhost/events/hello \
  -H "Content-Type: application/json" \
  -H "X-Correlation-ID: req-12345" \
  -d '{"world": "Event-driven write"}'

# Events published automatically to downstream systems
```

### 3. Stream Analytics

**Real-Time Event Stream Processing** - *R Pattern: Read-Only*
- **Read**: Consume events from Redis streams for analytics
- **No Write**: Cannot write back to database directly (read-only pattern)
- **Continuous**: Always-on stream processing with windowed operations
- Analytics and monitoring pattern for real-time insights

**pgstream (Change Data Capture)**
- Monitors PostgreSQL logical replication
- Captures INSERT, UPDATE, DELETE operations from both CRUD and Events paths
- Delivers unified change events via webhooks to `/webhook` endpoint

**Redis Streams Event Backbone**
- Durable message storage with exactly-once processing semantics
- Consumer groups enable multiple read-only consumers
- Uses `XREADGROUP` and `XACK` for guaranteed message processing

**SSE Streaming Endpoint**
- OpenResty/Lua endpoint at `/stream/sse/hello`
- Reads from Redis streams using consumer groups
- Provides Server-Sent Events for real-time consumption

**Arroyo Stream Processing**
- SQL-based stream processing engine for read-only analytics
- Performs windowed aggregations and joins on event streams
- Outputs analytical results back to PostgreSQL (via separate write path)

### 4. Service Mesh (Dapr)

**Circuit Breaking & Service Discovery**
- Dapr service mesh provides resilient service-to-service communication
- Circuit breaking for upstream services
- Service invocation through `/v1.0/invoke/service-name/method/endpoint`

**PubSub & State Management**
- Redis-based event bus for cross-service communication
- PostgreSQL state store with outbox pattern
- Automatic event routing and subscription management

## Data Flow Examples

### 1. CRUD Path (RWN - Read, Write, Now)
```bash
# Client creates record via CRUD path
POST /crud/hello {"world": "test"}
  â†“
# Read + Write + Now: Direct database operation via PostgREST  
INSERT INTO hello (world) VALUES ('test')
  â†“
# Now: Immediate HTTP response with ACID guarantees
{"id": 1, "world": "test"}
  â†“
# Background: CDC captures change for stream processing
pgstream â†’ Redis streams (source: "pgstream_cdc")
```

### 2. Events Path (RW - Read, Write Async)
```bash
# Client creates record via Events path
POST /events/hello {"world": "test"} + X-Correlation-ID: req-123
  â†“
# Read + Write: Dapr state management with outbox pattern
Dapr writes to PostgreSQL + publishes event atomically
  â†“
# Eventual response with correlation context preserved
{"id": 1, "world": "test"} + correlation headers
  â†“
# Automatic event publishing for downstream systems
CloudEvents with correlation_id, trace_id, etc.
```

### 3. Stream Analytics (R - Read-Only)
```bash
# Read: Events from both CRUD and Events paths flow to streams
XADD hello_stream * action I table_name hello data [...]
  â†“
# Read: SSE endpoint consumes with consumer group
XREADGROUP hello_stream arroyo_processors â†’ SSE
  â†“
# Read: Arroyo processes stream with SQL analytics
SELECT string_agg(world_value, ', ') as world
FROM hello_events
WHERE action = 'I' AND table_name = 'hello'
GROUP BY hop(interval '1' second, interval '5' second);
  â†“
# Read-only: Results written back via separate write mechanism
INSERT INTO grouphello (world) VALUES ('aggregated messages')
```

### Key Benefits of Single Event Source
- **No data corruption**: Single atomic write to Redis streams
- **Independent scaling**: Dapr and Arroyo can fail/restart independently  
- **Replay capability**: Each consumer group has its own offset
- **At-least-once delivery**: Redis consumer groups guarantee message processing

## Delivery Guarantees

### At-Least-Once Processing
- **Redis Consumer Groups**: Messages processed exactly once per consumer group
- **Acknowledgment Pattern**: `XACK` only after successful processing
- **Crash Recovery**: Unacknowledged messages remain in pending queue
- **No Message Loss**: Durable Redis streams persist until explicitly acknowledged

### Failure Scenarios
- **SSE Connection Drop**: Messages remain in Redis pending queue
- **Arroyo Restart**: Consumer group resumes from last acknowledged position
- **Database Failover**: pgstream reconnects and resumes from replication slot

## Development Workflow

### Starting the Stack
```bash
# Start core services
docker compose up -d database redis proxy

# Start CDC pipeline
docker compose up -d cdc

# Start stream processing
docker compose up -d arroyo

# Run integration tests
docker compose up e2e it
```

### Testing the Pipeline
```bash
# Test CRUD path - traditional synchronous operations
curl -X POST http://localhost/crud/hello \
  -H "Content-Type: application/json" \
  -d '{"world": "CRUD message"}'

# Test Events path - asynchronous with correlation
curl -X POST http://localhost/events/hello \
  -H "Content-Type: application/json" \
  -H "X-Correlation-ID: test-123" \
  -d '{"world": "Event message"}'

# Test Stream Analytics - real-time processing
curl -N http://localhost/stream/sse/hello

# Check aggregation results (from stream analytics)
curl http://localhost/crud/grouphello
```

### Monitoring
```bash
# Check Redis stream length
docker compose exec redis redis-cli XLEN hello_stream

# Monitor consumer group status
docker compose exec redis redis-cli XINFO GROUPS hello_stream

# View Arroyo web UI
open http://localhost:5115

# Access observability dashboards
open http://localhost:3000  # Grafana (admin/admin)
open http://localhost:9090  # Prometheus  
open http://localhost:16686 # Jaeger tracing
```

## Configuration

### Environment Variables
```bash
POSTGRES_USER=iris
POSTGRES_PASSWORD=postgres
POSTGRES_DB=tracker
UPSTREAM_HOST=app
UPSTREAM_PORT=8000
```

### Key Files
- `services/proxy/nginx.conf` - Proxy configuration with SSE endpoints
- `services/mesh/dapr/components/statestore.yaml` - Service mesh configuration
- `services/cdc/Dockerfile` - Change data capture setup
- `services/arroyo/queries/` - Stream processing SQL queries

## Advantages

1. **Three Clear Patterns**: CRUD (RWN), Events (RW), and Stream Analytics (R) with distinct capabilities
2. **Explicit Choice**: Developers choose CRUD (sync), Events (async), or Analytics (read-only) based on needs
3. **Unified Event Source**: Single Redis streams backbone feeds all downstream systems
4. **Lightweight**: Pure Redis streams, no heavy message brokers or complex coordination
5. **Guaranteed Delivery**: Consumer groups ensure exactly-once processing across all paths
6. **Real-time Analytics**: SSE provides low-latency streaming for continuous processing
7. **SQL-Based**: Familiar SQL syntax for both CRUD operations and stream processing
8. **Resilient**: Circuit breaking and automatic retry mechanisms built into Dapr mesh
9. **Scalable**: Independent horizontal scaling of CRUD, Events, and Analytics paths
10. **Observable**: Complete tracing and metrics with correlation guarantees on Events path

## Use Cases

### CRUD Path (RWN - Synchronous)
- **Traditional CRUD applications**: User interfaces requiring immediate feedback
- **Real-time operations**: Trading systems, reservation systems, inventory management
- **Interactive dashboards**: Live data display with strong consistency requirements

### Events Path (RW - Asynchronous)  
- **Event-driven microservices**: Business processes spanning multiple systems
- **Audit trails**: Compliance and traceability with correlation guarantees
- **Workflow orchestration**: Multi-step processes with eventual consistency
- **Integration patterns**: Connecting heterogeneous systems with guaranteed delivery

### Stream Analytics (R - Read-Only)
- **Real-time analytics**: Live dashboards and KPI monitoring from database changes
- **Stream aggregation**: Windowed computations and moving averages
- **IoT processing**: Real-time sensor data aggregation and alerting
- **Anomaly detection**: Pattern recognition and automated alerting systems

## Alternative Ultra-Lightweight Architecture

For maximum efficiency, Mecha can be dramatically optimized by replacing heavy components with modern, lightweight alternatives:

### **Size Comparison**
```
Current Architecture:     1.77GB
Ultra-Lightweight:        240MB
Reduction:               86% smaller
```

### **Component Replacements**

| Component | Current | Alternative | Size Reduction |
|-----------|---------|-------------|----------------|
| **Database** | PostgreSQL (837MB) + pgstream (338MB) | Turso/libSQL (15MB) + Native CDC | **96% smaller** |
| **CRUD API** | PostgREST (442MB) | prest (20MB) | **95% smaller** |
| **Proxy** | OpenResty (159MB) | HAProxy + Lua (20MB) | **87% smaller** |
| **Webhooks** | Custom Lua | webhook/webhook (8MB) | **Specialized** |
| **SSE Streaming** | Custom Lua | Mercure (8MB) | **Specialized** |
| **Stream Processing** | Arroyo (318MB) | RisingWave (100MB) | **69% smaller** |
| **Event Store** | Redis (69MB) | Redis (69MB) | **Same** |

### **Ultra-Lightweight Stack (240MB total)**

```yaml
# docker-compose-lightweight.yml
services:
  # Database with native CDC (15MB)
  database:
    image: libsql/sqld:latest
    environment:
      SQLD_HTTP_LISTEN_ADDR: "0.0.0.0:8080"
      SQLD_ENABLE_WEBSOCKETS: "true"
    volumes:
      - ./schema.sql:/docker-entrypoint-initdb.d/schema.sql

  # Auto-generated REST API (20MB)
  crud:
    image: prest/prest:latest
    environment:
      PREST_PG_URL: "http://database:8080"
      PREST_HTTP_PORT: 3000
    depends_on: [database]

  # Webhook processor (8MB)
  webhook:
    image: webhook/webhook:latest
    volumes:
      - ./hooks.json:/etc/webhook/hooks.json
    command: ["-hooks=/etc/webhook/hooks.json", "-verbose"]

  # SSE streaming server (8MB)
  sse:
    image: dunglas/mercure:latest
    environment:
      MERCURE_PUBLISHER_JWT_KEY: 'your-secret-key'
      MERCURE_SUBSCRIBER_JWT_KEY: 'your-secret-key'
      MERCURE_EXTRA_DIRECTIVES: |
        cors_origins *

  # Lightweight proxy with correlation IDs (20MB)
  proxy:
    image: haproxy:2.8-alpine
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
      - ./correlation.lua:/usr/local/etc/haproxy/correlation.lua
    ports: ["80:80"]
    depends_on: [crud, webhook, sse]

  # SQL-based stream processing (100MB)
  stream:
    image: risingwavelabs/risingwave:latest
    environment:
      RW_META_ADDR: "http://stream:5690"
    volumes:
      - ./stream-queries:/queries
    depends_on: [sse]

  # Event backbone (69MB)
  redis:
    image: redis:7.4.1-alpine
    volumes: ["/data"]
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
```

### **Key Ultra-Lightweight Features**

#### **1. Turso Native CDC**
```sql
-- Native change data capture in libSQL
CREATE CHANGEFEED FOR hello 
INTO 'webhook-http://webhook:9000/hooks/turso-cdc'
WITH format = 'json';
```

#### **2. HAProxy Correlation IDs**
```lua
-- correlation.lua - Automatic correlation ID injection
core.register_action("set_correlation_id", {"http-req"}, function(txn)
    local correlation_id = txn.http:req_get_headers()["x-correlation-id"]
    if correlation_id == nil then
        local uuid = require("uuid")
        correlation_id = uuid.new()
        txn.http:req_add_header("X-Correlation-ID", correlation_id)
    end
end)
```

#### **3. RisingWave Stream Processing**
```sql
-- SQL-first streaming with windowed aggregation
CREATE SOURCE hello_events FROM WEBHOOK
BODY FORMAT JSON;

CREATE MATERIALIZED VIEW hello_aggregated AS
SELECT 
    string_agg(world_value, ', ') as world,
    COUNT(*) as event_count,
    window_start,
    window_end
FROM TUMBLE(hello_events, timestamp, INTERVAL '5 second')
WHERE table_name = 'hello' AND action = 'INSERT'
GROUP BY window_start, window_end;
```

#### **4. Webhook Configuration**
```json
// hooks.json - Turso CDC webhook handler
[{
  "id": "turso-cdc",
  "execute-command": "/forward-to-redis.sh",
  "command-working-directory": "/tmp",
  "pass-arguments-to-command": [
    {"source": "payload", "name": "data"}
  ],
  "response-headers": [
    {"name": "X-Correlation-ID", "value": "$CORRELATION_ID"}
  ]
}]
```

### **Benefits of Ultra-Lightweight Architecture**

1. **ğŸš€ Massive Size Reduction**: 86% smaller (1.77GB â†’ 240MB)
2. **âš¡ Faster Startup**: All services start in under 30 seconds
3. **ğŸ’° Lower Resource Usage**: Runs comfortably on 1GB RAM
4. **ğŸ”§ Modern Stack**: Latest cloud-native technologies
5. **ğŸ“Š Same Functionality**: Maintains all three-path capabilities
6. **ğŸ›¡ï¸ Production Ready**: All components are battle-tested

### **Trade-offs**

- **Smaller ecosystem**: Some components have fewer plugins than PostgreSQL/nginx
- **Newer technologies**: libSQL and RisingWave are less mature than PostgreSQL/Arroyo
- **Learning curve**: Different configuration syntax for HAProxy vs nginx
- **Feature gaps**: Some advanced PostgreSQL features not available in libSQL

### **Migration Path**

1. **Phase 1**: Replace PostgREST with prest (420MB savings)
2. **Phase 2**: Switch to Turso + native CDC (1.1GB savings)  
3. **Phase 3**: Replace OpenResty with HAProxy (139MB savings)
4. **Phase 4**: Add specialized webhook/SSE services
5. **Phase 5**: Switch to RisingWave stream processing (218MB savings)

This ultra-lightweight variant is ideal for:
- **Development environments**
- **Edge deployments** 
- **Resource-constrained environments**
- **Cost-sensitive production workloads**
